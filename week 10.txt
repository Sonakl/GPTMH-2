ACTIVITY :-10
Regression - Rebuild with machine learning model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import load_breast_cancer
# Load the breast cancer dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target  # Adding the target variable
# Select features and target variable
# We'll predict the 'mean radius' using some features
X = df[['mean texture', 'mean perimeter', 'mean area']]  # Example features
y = df['mean radius']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)
# Make predictions
y_pred = model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Plotting
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2, label='Ideal Fit')  # Diagonal line
plt.title('Actual vs Predicted Mean Radius')
plt.xlabel('Actual Mean Radius')
plt.ylabel('Predicted Mean Radius')
plt.legend()
plt.show()
output : 
Mean Squared Error: 0.044717685824075974
R-squared: 0.9963001166958553
 
Regression - Rebuild with deep learning model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import mean_squared_error, r2_score
# Step 1: Load the dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target  # Adding the target variable
# Step 2: Select features and target variable
X = df[['mean texture', 'mean perimeter', 'mean area']]  # Example features
y = df['mean radius']
# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Step 4: Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Step 5: Build the neural network model
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer
model.add(Dense(32, activation='relu'))  # Hidden layer
model.add(Dense(1))  # Output layer for regression
# Step 6: Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')
# Step 7: Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)
# Step 8: Make predictions
y_pred = model.predict(X_test)
# Step 9: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Step 10: Plotting
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2, label='Ideal Fit')  # Diagonal line
plt.title('Actual vs Predicted Mean Radius')
plt.xlabel('Actual Mean Radius')
plt.ylabel('Predicted Mean Radius')
plt.legend()
plt.show()
# Optional: Visualize the loss over epochs
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
output :
23/23 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - loss: 203.0524 - val_loss: 189.5856
Epoch 2/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 173.4150 - val_loss: 152.9895
Epoch 3/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 140.8854 - val_loss: 100.0990
Epoch 4/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 90.7587 - val_loss: 48.5967
Epoch 5/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 41.3421 - val_loss: 28.4803
Epoch 6/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 26.1682 - val_loss: 29.6214
Epoch 7/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 19.7573 - val_loss: 26.9577
Epoch 8/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 17.5317 - val_loss: 22.6396
Epoch 9/100
Mean Squared Error: 0.03801231849033416
R-squared: 0.9968549100888737
 
 
Classification - Rebuild with machine learning model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import load_breast_cancer
# Step 1: Load the dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target  # Adding the target variable (0 = malignant, 1 = benign)
# Step 2: Select features and target variable
X = df[['mean texture', 'mean perimeter', 'mean area']]  # Example features
y = df['target']
# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Step 4: Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Step 5: Create and train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)
# Step 6: Make predictions
y_pred = model.predict(X_test)
# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)
# Step 8: Plotting the Confusion Matrix
plt.matshow(conf_matrix, cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
output :
Accuracy: 0.9298245614035088
Confusion Matrix:
[[38  5]
 [ 3 68]]
Classification Report:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90        43
           1       0.93      0.96      0.94        71

    accuracy                           0.93       114
   macro avg       0.93      0.92      0.92       114
weighted avg       0.93      0.93      0.93       114

Text(0.5, 1.0, 'Confusion Matrix')
 

Classification - Rebuild with deep learning model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import load_breast_cancer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# Step 1: Load the dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target  # Adding the target variable (0 = malignant, 1 = benign)
# Step 2: Select features and target variable
X = df[['mean texture', 'mean perimeter', 'mean area']]  # Example features
y = df['target']
# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Step 4: Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Step 5: Build the neural network model
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer with 64 neurons
model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons
model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron (for binary classification)
# Step 6: Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Step 7: Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)
# Step 8: Make predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32")  # Convert probabilities to binary output
# Step 9: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)
# Step 10: Plotting the Confusion Matrix
plt.matshow(conf_matrix, cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
# Add labels and ticks
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
# Add class labels to the confusion matrix
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['Malignant', 'Benign'])
plt.yticks(tick_marks, ['Malignant', 'Benign'])
# Add text annotations
for i in range(2):
    for j in range(2):
        plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black')
# Display the plot
plt.colorbar()
plt.show()
# Optional: Visualize the training loss and accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
output :
23/23 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8962 - loss: 0.5507 - val_accuracy: 0.8791 - val_loss: 0.4060
Epoch 2/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9116 - loss: 0.3663 - val_accuracy: 0.8681 - val_loss: 0.3108
Epoch 3/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9161 - loss: 0.2717 - val_accuracy: 0.8791 - val_loss: 0.2833
Epoch 4/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8964 - loss: 0.2611 - val_accuracy: 0.8571 - val_loss: 0.2807
Epoch 5/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8796 - loss: 0.2694 - val_accuracy: 0.8681 - val_loss: 0.2730
Epoch 6/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9015 - loss: 0.2678 - val_accuracy: 0.8681 - val_loss: 0.2738
Epoch 7/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9164 - loss: 0.2065 - val_accuracy: 0.8462 - val_loss: 0.2768
Epoch 8/100
23/23 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9050 - loss: 0.2245 - val_accuracy: 0.8571 - val_loss: 0.2731
Epoch 9/100
Accuracy: 0.9210526315789473
Confusion Matrix:
[[39  4]
 [ 5 66]]
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.91      0.90        43
           1       0.94      0.93      0.94        71

    accuracy                           0.92       114
   macro avg       0.91      0.92      0.92       114
weighted avg       0.92      0.92      0.92       114
 
 

Analyze the performance of ML and DL
1. *Handling Complex Patterns*: Deep learning models, particularly neural networks, can capture complex patterns and relationships in large datasets that traditional algorithms may struggle with.

2. *Feature Learning*: Deep learning can automatically extract and learn features from raw data (e.g., images, text), reducing the need for manual feature engineering.

3. *Scalability*: Deep learning models often scale better with large datasets. As the amount of data increases, their performance typically improves.

4. *Performance on Unstructured Data*: Deep learning excels at processing unstructured data (e.g., images, audio, text) due to its architecture, which is designed to work with high-dimensional input.

5. *State-of-the-Art Results*: For many tasks, such as image recognition, natural language processing, and speech recognition, deep learning models have achieved state-of-the-art performance.

6. *Transfer Learning*: Pre-trained deep learning models can be fine-tuned for specific tasks, making them more efficient in terms of training time and data requirements.

7. *End-to-End Learning*: Deep learning allows for end-to-end learning, meaning that the entire pipeline (feature extraction to prediction) can be trained simultaneously, which can improve overall performance.

8. *Robustness*: Deep learning models can be more robust to noise and variations in data, as they can learn high-level abstractions.

9. *Advanced Architectures*: Deep learning allows for the use of advanced architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which are tailored for specific types of data.

10. *Continuous Improvement*: With ongoing advancements in deep learning techniques and architectures, models are continually improving, leading to better performance in various applications.


 Upload documents to GitHub, follow these steps:
### 1. Create a Repository
1. *Sign in* to your GitHub account.
2. Click on the *+ icon* in the upper right corner and select *New repository*.
3. Fill in the repository name, description, and choose whether it will be public or private.
4. Click *Create repository*.

### 2. Upload Documents
*Method 1: Using GitHub Website*
1. Go to your repository.
2. Click on the *Add file* button, then select *Upload files*.
3. Drag and drop your documents or click *choose your files* to select them from your computer.
4. Add a commit message describing your upload.
5. Click *Commit changes*.

To copy and share the link to your GitHub repository or a specific file, follow these steps:
### Copying the Link to Your Repository
1. *Open your repository* on GitHub.
2. *Copy the URL* from the address bar of your browser. It will look something like https://github.com/Sonakl/GPTMH-2.git.

### Copying the Link to a Specific File
1. *Navigate to the file* in your repository.
2. Click on the file to open it.
3. *Copy the URL* from the address bar. It will look like (adjust main if your branch name is different).
### Sharing the Link
- *Paste the link* wherever you want to share it (e.g., email, chat, social media).
- You can use URL shorteners (like Bitly) for cleaner links if necessary.
That's it! You can now easily share your GitHub repository or specific files.

